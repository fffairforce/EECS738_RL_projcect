{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "treasure_hunt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMobUuDjNG4l"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXT6VlpYNQFi"
      },
      "source": [
        "# Q-learning project\n",
        "This treasure hunt game is build in order to implement reinforcement learning algorithm. A treasure map of 5x5 table is designed, with several obstacle and 1 monster and 1 treasure at end. Q-Learning algorithm is used to predict the efficient moves in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6jl0I3aNCba"
      },
      "source": [
        "#Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsqgxxMWOUpX"
      },
      "source": [
        "# initialize learning rate\n",
        "learning_rate = 0.8\n",
        "\n",
        "# import analog map "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhBFoeM2OZ1C"
      },
      "source": [
        "Reward table consist of actions(row) on each position(column) with a given reward. This is fixed if we have a fixed map with certain obstacles and monter and treasure.\n",
        "\n",
        "position matrix is given as order shown.\n",
        "\n",
        "Defining states as in Q-Table where columns are the actions and the rows are the states.\n",
        "\n",
        "[add pic]\n",
        "\n",
        "Each Q-table score will be the maximum expected future reward to get if it takes that action at that state.Improvements on Q-Table would be made during each iteraction.\n",
        "\n",
        "Q-Table is initialized with 25x5 array of zeros where each row is a state and 5 columns correspond to the actions up/down/left/right/neutral as is represented in reward table.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAQOILpFPxjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c530687e-9c35-48b2-a409-f5d33aec238f"
      },
      "source": [
        " # initialize Q-table\n",
        " # specify 5 actions in order(U, D, L, R, N)\n",
        " # reward for blank box 1, obstacle -5, monster -10, treasure +10, no move 0\n",
        " reward_table = np.array([[0, 1, 0, -5, 1],\n",
        "              [0, -5, 1, 1, -5],\n",
        "              [0, 1, -5, 1, 1],\n",
        "              [0, 1, 1, 10, 1],\n",
        "              [0, -5, 1, 0, 10],\n",
        "              [1, 1, 0, -5, 1],\n",
        "              [-5, 1, 1, 1, -5],\n",
        "              [1, -5, -5, 1, 1],\n",
        "              [1, 1, 1, -5, 1],\n",
        "              [10, -10, 1, 0, -5],\n",
        "              [1, 1, 0, 1, 1],\n",
        "              [-5, 1, 1, -5, 1],\n",
        "              [1, 1, 1, 1, 1, -5],\n",
        "              [1, 1, -5, -10, 1],\n",
        "              [-5, 1, 1, 0, -10],\n",
        "              [1, 1, 0, 1, 1],\n",
        "              [1, 1, 1, 1, 1],\n",
        "              [-5, -5, 1, 1, 1],\n",
        "              [1, 1, 1, 1, 1],\n",
        "              [-10, -5, 1, 0, 1],\n",
        "              [1, 0, 0, 1, 1],\n",
        "              [1, 0, 1, -5, 1],\n",
        "              [1, 0, 1, 1, -5],\n",
        "              [1, 0, -5, -5, 1],\n",
        "              [1, 0, 1, 0, -5]                        \n",
        "              ])\n",
        " q_table = np.zeros((25,5))\n",
        " #q_table\n",
        " #reward_table"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([0, 1, 0, -5, 1]), list([0, -5, 1, 1, -5]),\n",
              "       list([0, 1, -5, 1, 1]), list([0, 1, 1, 10, 1]),\n",
              "       list([0, -5, 1, 0, 10]), list([1, 1, 0, -5, 1]),\n",
              "       list([-5, 1, 1, 1, -5]), list([1, -5, -5, 1, 1]),\n",
              "       list([1, 1, 1, -5, 1]), list([10, -10, 1, 0, -5]),\n",
              "       list([1, 1, 0, 1, 1]), list([-5, 1, 1, -5, 1]),\n",
              "       list([1, 1, 1, 1, 1, -5]), list([1, 1, -5, -10, 1]),\n",
              "       list([-5, 1, 1, 0, -10]), list([1, 1, 0, 1, 1]),\n",
              "       list([1, 1, 1, 1, 1]), list([-5, -5, 1, 1, 1]),\n",
              "       list([1, 1, 1, 1, 1]), list([-10, -5, 1, 0, 1]),\n",
              "       list([1, 0, 0, 1, 1]), list([1, 0, 1, -5, 1]),\n",
              "       list([1, 0, 1, 1, -5]), list([1, 0, -5, -5, 1]),\n",
              "       list([1, 0, 1, 0, -5])], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TXtQG1Zim8i"
      },
      "source": [
        "# Initialize Transition Table\n",
        "\n",
        "The transition table lays out all the possible movements to another state given the current state. The invalid move is marked as -1 and no movement is marked as the corresponding state number. The transition table will be referred to for tracing out the path to treasure chest through the maze where value represents positions of the map.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1rgod8ZihsO"
      },
      "source": [
        "#Initialize the transition matrix\n",
        "transition_table = np.array([[-1, 5, -1, 1, 0],\n",
        "              [-1, 6, 0, 2, 1],\n",
        "              [-1, 7, 1, 3, 2],\n",
        "              [-1, 8, 2, 4, 3],\n",
        "              [-1, 9, 3, -1, 4],\n",
        "              [0, 10, -1, 6, 5],\n",
        "              [1, 11, 5, 7, 6],\n",
        "              [2, 12, 6, 8, 7],\n",
        "              [3, 13, 7, 9, 8],\n",
        "              [4, 14, 8, -1, 9],\n",
        "              [5, 15, -1, 11, 10],\n",
        "              [6, 16, 10, 12, 11],\n",
        "              [7, 17, 11, 13, 12],\n",
        "              [8, 18, 12, 14, 13],\n",
        "              [9, 19, 13, -1, 14],\n",
        "              [10, 20, -1, 16, 15],\n",
        "              [11, 21, 15, 17, 16],\n",
        "              [12, 22, 16, 18, 17],\n",
        "              [13, 23, 17, 19, 18],\n",
        "              [14, 24, 18, -1, 19],\n",
        "              [15, -1, -1, 21, 20],\n",
        "              [16, -1, 20, 22, 21],\n",
        "              [17, -1, 21, 23, 22],\n",
        "              [18, -1, 22, 24, 23],\n",
        "              [19, -1, 23, -1, 24]\n",
        "              ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoS9LzLJir1J"
      },
      "source": [
        "#Initialize Action Table\n",
        "\n",
        "The action table represents the valid actions that can be taken by the agent at each position(0-24 in columns). Here 'up, down, left, right and no action' are encoded as 0, 1, 2, 3 and 4 respectively. this table should be checked before each action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPUVZNDwiu8E",
        "outputId": "a8759d48-4ce1-4c6f-a2e6-10f5ca315cb9"
      },
      "source": [
        "#Initialize the action\n",
        "#0-up, 1-down, 2-left, 3-right, 4-No action\n",
        "action_table = np.array([[1, 3, 4],\n",
        "            [1, 2, 3, 4],\n",
        "            [1, 2, 3, 4],\n",
        "            [1, 2, 3, 4],\n",
        "            [1, 2, 4],\n",
        "            [0, 1, 3, 4],\n",
        "            [0, 1, 2, 3, 4],\n",
        "            [0, 1, 2, 3, 4],\n",
        "            [0, 1, 2, 3, 4],\n",
        "            [0, 1, 2, 4],\n",
        "            [0, 1, 3, 4],\n",
        "            [0, 1, 2, 3, 4],\n",
        "            [0, 1, 2, 3, 4],\n",
        "            [0, 1, 2, 3, 4],\n",
        "            [0, 1, 2, 4],\n",
        "            [0, 1, 3, 4],\n",
        "            [0, 1, 2, 3, 4],\n",
        "            [0, 1, 2, 3, 4],\n",
        "            [0, 1, 2, 3, 4],\n",
        "            [0, 1, 2, 4],\n",
        "            [0, 3, 4],\n",
        "            [0, 2, 3, 4],\n",
        "            [0, 2, 3, 4],\n",
        "            [0, 2, 3, 4],\n",
        "            [0, 2, 4]\n",
        "            ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDeD7FXXnRJe"
      },
      "source": [
        "# Running reinforcement learning\n",
        "\n",
        "Q-Learning attempts to learn the value of being in a given state, and taking a specific action there.\n",
        "\n",
        "Bellman Equation is used here to update our Q-Table which is initialized above. Q(s,a) = r + γ(max(Q(ś,á))\n",
        "\n",
        "Updated Q(state,action)←(1−α)Q(state,action)+α(reward γmaxaQ(next state,all actions))\n",
        "\n",
        "' α ' is the learning rate. ‘ s ’ is current state, ‘ a ’ is the action the agent takes from the current state, ‘ ś ’ represents the state resulting from the action. ‘ r ’ is the reward for taking the action and ‘ γ ’ is the discount factor. So, the Q value for the state ‘ ś ’ taking the action ‘ á ’ is the sum of the instant reward and the discounted future reward (value of the resulting state). The discount factor ‘ γ ’ determines the importance of future rewards. \n",
        "\n",
        "to sum up the process of Q-learning:\n",
        "\n",
        "* Initialize the Q-table by all zeros.\n",
        "* Start exploring actions: For each state, select any one among all possible actions for the current state (S).\n",
        "* Travel to the next state (S') as a result of that action (a).\n",
        "* For all possible actions from the state (S') select the one with the highest Q-value.\n",
        "* Update Q-table values using the equation.\n",
        "* Set the next state as the current state.\n",
        "* If goal state is reached, then end and repeat the process.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhzUsdDinTxm"
      },
      "source": [
        "# The agent will be trained for 500 iterations\n",
        "# The iterations will run until all the values become constant\n",
        "\n",
        "for i in range(500):\n",
        "    #Start with the first most state\n",
        "    start = 0\n",
        "    current = start\n",
        "    \n",
        "    #Keep moving forward until the goal state is reached\n",
        "    while current != 24:\n",
        "        #Select one among all possible actions for the current state\n",
        "        action = random.choice(action_table[current])\n",
        "        \n",
        "        #Travel to the next state as a result of the action taken previously\n",
        "        next_state = transition_table[current][action]\n",
        "        future_reward = []\n",
        "        \n",
        "        #Add all the current rewards value for all possible actions\n",
        "        for action_next in action_table[next_state]:\n",
        "            future_reward.append(q_table[next_state][action_next])\n",
        "        \n",
        "        #For all possible actions from the next state, select the one with highest Q value\n",
        "        q_state = reward_table[current][action] + learning_rate*max(future_reward)\n",
        "        \n",
        "        #Update the Q table with new reward value\n",
        "        q_table[current][action] = q_state\n",
        "        \n",
        "        #Set the next state as the current state in order to move forward\n",
        "        current = next_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsH7sucQ2djB",
        "outputId": "01821c27-5684-4e4f-e148-a9cc3df57ab9"
      },
      "source": [
        "#The final Q-table with all updated reward values\n",
        "q_table"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.       , 15.9056   ,  0.       , 22.04     , 18.632    ],\n",
              "       [ 0.       , 17.432    , 18.632    , 33.8      , 22.04     ],\n",
              "       [ 0.       , 28.04     , 22.04     , 41.       , 33.8      ],\n",
              "       [ 0.       , 33.8      , 33.8      , 50.       , 41.       ],\n",
              "       [ 0.       , 35.       , 41.       ,  0.       , 50.       ],\n",
              "       [18.632    , 13.72448  ,  0.       , 17.432    , 15.9056   ],\n",
              "       [22.04     , 14.9456   , 15.9056   , 28.04     , 17.432    ],\n",
              "       [33.8      , 17.432    , 17.432    , 33.8      , 28.04     ],\n",
              "       [41.       , 28.04     , 28.04     , 35.       , 33.8      ],\n",
              "       [50.       , 18.       , 33.8      ,  0.       , 35.       ],\n",
              "       [15.9056   , 14.437184 ,  0.       , 14.9456   , 13.72448  ],\n",
              "       [17.432    , 16.79648  , 13.72448  , 17.432    , 14.9456   ],\n",
              "       [28.04     , 19.7456   , 14.9456   , 28.04     , 23.432    ],\n",
              "       [33.8      , 23.432    , 17.432    , 18.       , 28.04     ],\n",
              "       [35.       , 19.7456   , 28.04     ,  0.       , 18.       ],\n",
              "       [13.72448  , 12.5497472,  0.       , 16.79648  , 14.437184 ],\n",
              "       [14.9456   , 14.437184 , 14.437184 , 19.7456   , 16.79648  ],\n",
              "       [17.432    , 10.79648  , 16.79648  , 23.432    , 19.7456   ],\n",
              "       [28.04     , 19.7456   , 19.7456   , 19.7456   , 23.432    ],\n",
              "       [18.       , -5.       , 23.432    ,  0.       , 19.7456   ],\n",
              "       [14.437184 ,  0.       ,  0.       , 14.437184 , 12.5497472],\n",
              "       [16.79648  ,  0.       , 12.5497472, 10.79648  , 14.437184 ],\n",
              "       [19.7456   ,  0.       , 14.437184 , 19.7456   , 10.79648  ],\n",
              "       [23.432    ,  0.       , 10.79648  , -5.       , 19.7456   ],\n",
              "       [ 0.       ,  0.       ,  0.       ,  0.       ,  0.       ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCxlcfx02iWr"
      },
      "source": [
        "# Result\n",
        "show the best route to seek treasure referring to the final Q-table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "e4-MWGhX2hj2",
        "outputId": "9490bd95-3adc-4ebf-9520-2ebae3549c04"
      },
      "source": [
        "#Convert to dataframe for better visualization\n",
        "q_df = pd.DataFrame(q_table)\n",
        "q_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.905600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22.040000</td>\n",
              "      <td>18.632000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>17.432000</td>\n",
              "      <td>18.632000</td>\n",
              "      <td>33.800000</td>\n",
              "      <td>22.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>28.040000</td>\n",
              "      <td>22.040000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>33.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.800000</td>\n",
              "      <td>33.800000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>41.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>50.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>18.632000</td>\n",
              "      <td>13.724480</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>17.432000</td>\n",
              "      <td>15.905600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>22.040000</td>\n",
              "      <td>14.945600</td>\n",
              "      <td>15.905600</td>\n",
              "      <td>28.040000</td>\n",
              "      <td>17.432000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>33.800000</td>\n",
              "      <td>17.432000</td>\n",
              "      <td>17.432000</td>\n",
              "      <td>33.800000</td>\n",
              "      <td>28.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>41.000000</td>\n",
              "      <td>28.040000</td>\n",
              "      <td>28.040000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>33.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>50.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>33.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>15.905600</td>\n",
              "      <td>14.437184</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.945600</td>\n",
              "      <td>13.724480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>17.432000</td>\n",
              "      <td>16.796480</td>\n",
              "      <td>13.724480</td>\n",
              "      <td>17.432000</td>\n",
              "      <td>14.945600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>28.040000</td>\n",
              "      <td>19.745600</td>\n",
              "      <td>14.945600</td>\n",
              "      <td>28.040000</td>\n",
              "      <td>23.432000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>33.800000</td>\n",
              "      <td>23.432000</td>\n",
              "      <td>17.432000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>28.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>35.000000</td>\n",
              "      <td>19.745600</td>\n",
              "      <td>28.040000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>18.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>13.724480</td>\n",
              "      <td>12.549747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>16.796480</td>\n",
              "      <td>14.437184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>14.945600</td>\n",
              "      <td>14.437184</td>\n",
              "      <td>14.437184</td>\n",
              "      <td>19.745600</td>\n",
              "      <td>16.796480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17.432000</td>\n",
              "      <td>10.796480</td>\n",
              "      <td>16.796480</td>\n",
              "      <td>23.432000</td>\n",
              "      <td>19.745600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>28.040000</td>\n",
              "      <td>19.745600</td>\n",
              "      <td>19.745600</td>\n",
              "      <td>19.745600</td>\n",
              "      <td>23.432000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>18.000000</td>\n",
              "      <td>-5.000000</td>\n",
              "      <td>23.432000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>19.745600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>14.437184</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.437184</td>\n",
              "      <td>12.549747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>16.796480</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.549747</td>\n",
              "      <td>10.796480</td>\n",
              "      <td>14.437184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>19.745600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.437184</td>\n",
              "      <td>19.745600</td>\n",
              "      <td>10.796480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23.432000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.796480</td>\n",
              "      <td>-5.000000</td>\n",
              "      <td>19.745600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0          1          2          3          4\n",
              "0    0.000000  15.905600   0.000000  22.040000  18.632000\n",
              "1    0.000000  17.432000  18.632000  33.800000  22.040000\n",
              "2    0.000000  28.040000  22.040000  41.000000  33.800000\n",
              "3    0.000000  33.800000  33.800000  50.000000  41.000000\n",
              "4    0.000000  35.000000  41.000000   0.000000  50.000000\n",
              "5   18.632000  13.724480   0.000000  17.432000  15.905600\n",
              "6   22.040000  14.945600  15.905600  28.040000  17.432000\n",
              "7   33.800000  17.432000  17.432000  33.800000  28.040000\n",
              "8   41.000000  28.040000  28.040000  35.000000  33.800000\n",
              "9   50.000000  18.000000  33.800000   0.000000  35.000000\n",
              "10  15.905600  14.437184   0.000000  14.945600  13.724480\n",
              "11  17.432000  16.796480  13.724480  17.432000  14.945600\n",
              "12  28.040000  19.745600  14.945600  28.040000  23.432000\n",
              "13  33.800000  23.432000  17.432000  18.000000  28.040000\n",
              "14  35.000000  19.745600  28.040000   0.000000  18.000000\n",
              "15  13.724480  12.549747   0.000000  16.796480  14.437184\n",
              "16  14.945600  14.437184  14.437184  19.745600  16.796480\n",
              "17  17.432000  10.796480  16.796480  23.432000  19.745600\n",
              "18  28.040000  19.745600  19.745600  19.745600  23.432000\n",
              "19  18.000000  -5.000000  23.432000   0.000000  19.745600\n",
              "20  14.437184   0.000000   0.000000  14.437184  12.549747\n",
              "21  16.796480   0.000000  12.549747  10.796480  14.437184\n",
              "22  19.745600   0.000000  14.437184  19.745600  10.796480\n",
              "23  23.432000   0.000000  10.796480  -5.000000  19.745600\n",
              "24   0.000000   0.000000   0.000000   0.000000   0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WTJCsvgApAD"
      },
      "source": [
        "ref:\n",
        "\n",
        "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
        "\n",
        "https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/"
      ]
    }
  ]
}